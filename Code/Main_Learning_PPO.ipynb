{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from printind.printind_function import printi, printiv\n",
    "from Env_instance import resume_env, nb_actuations\n",
    "\n",
    "import numpy as np\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printi(\"resume env\")\n",
    "\n",
    "environment = resume_env(plot=False, step=100, dump=100, first_run=False)\n",
    "deterministic=True\n",
    "\n",
    "printi(\"define network specs\")\n",
    "\n",
    "network_spec = [\n",
    "    dict(type='dense', size=512),\n",
    "    dict(type='dense', size=512),\n",
    "]\n",
    "\n",
    "printi(\"define agent\")\n",
    "\n",
    "printiv(environment.states)\n",
    "printiv(environment.actions)\n",
    "printiv(network_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printi(\"resume env\")\n",
    "\n",
    "environment = resume_env(plot=2000, step=2000, dump=100)\n",
    "deterministic=True\n",
    "\n",
    "printi(\"define network specs\")\n",
    "\n",
    "network_spec = [\n",
    "    dict(type='dense', size=512),\n",
    "    dict(type='dense', size=512),\n",
    "]\n",
    "\n",
    "printi(\"define agent\")\n",
    "\n",
    "printiv(environment.states)\n",
    "printiv(environment.actions)\n",
    "printiv(network_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(\n",
    "    states=environment.states(),\n",
    "    actions=environment.actions(),\n",
    "    network=network_spec,\n",
    "    # Agent\n",
    "    states_preprocessing=None,\n",
    "    actions_exploration=None,\n",
    "    reward_preprocessing=None,\n",
    "    # MemoryModel\n",
    "    update_mode=dict(\n",
    "        unit='episodes',\n",
    "        # 10 episodes per update\n",
    "        batch_size=20,\n",
    "        # Every 10 episodes\n",
    "        frequency=20\n",
    "    ),\n",
    "    memory=dict(\n",
    "        type='latest',\n",
    "        include_next_states=False,\n",
    "        capacity=10000\n",
    "    ),\n",
    "    # DistributionModel\n",
    "    distributions=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode='states',\n",
    "    baseline=dict(\n",
    "        type='mlp',\n",
    "        sizes=[32, 32]\n",
    "    ),\n",
    "    baseline_optimizer=dict(\n",
    "        type='multi_step',\n",
    "        optimizer=dict(\n",
    "            type='adam',\n",
    "            learning_rate=1e-3\n",
    "        ),\n",
    "        num_steps=5\n",
    "    ),\n",
    "    gae_lambda=0.97,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    subsampling_fraction=0.2,\n",
    "    optimization_steps=25,\n",
    "    execution=dict(\n",
    "        type='single',\n",
    "        session_config=None,\n",
    "        distributed_spec=None\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "\n",
    "    printi(\"save the mode\")\n",
    "\n",
    "    name_save = \"./saved_models/ppo_model\"\n",
    "    r.agent.save_model(name_save, append_timestep=False)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(agent=agent, environment=environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run(episodes=250, max_episode_timesteps=nb_actuations, episode_finished=episode_finished)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
