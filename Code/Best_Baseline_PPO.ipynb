{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just defining same as main_learning_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from printind.printind_function import printi, printiv\n",
    "from Env_instance import resume_env, nb_actuations\n",
    "\n",
    "import numpy as np\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printi(\"resume env\")\n",
    "\n",
    "environment = resume_env(plot=2000, step=100, dump=100, first_run=False)\n",
    "deterministic=True\n",
    "\n",
    "printi(\"define network specs\")\n",
    "\n",
    "network_spec = [\n",
    "    dict(type='dense', size=512),\n",
    "    dict(type='dense', size=512),\n",
    "]\n",
    "\n",
    "printi(\"define agent\")\n",
    "\n",
    "printiv(environment.states)\n",
    "printiv(environment.actions)\n",
    "printiv(network_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(\n",
    "    states=environment.states(),\n",
    "    actions=environment.actions(),\n",
    "    network=network_spec,\n",
    "    # Agent\n",
    "    states_preprocessing=None,\n",
    "    actions_exploration=None,\n",
    "    reward_preprocessing=None,\n",
    "    # MemoryModel\n",
    "    update_mode=dict(\n",
    "        unit='episodes',\n",
    "        # 10 episodes per update\n",
    "        batch_size=20,\n",
    "        # Every 10 episodes\n",
    "        frequency=20\n",
    "    ),\n",
    "    memory=dict(\n",
    "        type='latest',\n",
    "        include_next_states=False,\n",
    "        capacity=10000\n",
    "    ),\n",
    "    # DistributionModel\n",
    "    distributions=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode='states',\n",
    "    baseline=dict(\n",
    "        type='mlp',\n",
    "        sizes=[32, 32]\n",
    "    ),\n",
    "    baseline_optimizer=dict(\n",
    "        type='multi_step',\n",
    "        optimizer=dict(\n",
    "            type='adam',\n",
    "            learning_rate=1e-3\n",
    "        ),\n",
    "        num_steps=5\n",
    "    ),\n",
    "    gae_lambda=0.97,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    subsampling_fraction=0.2,\n",
    "    optimization_steps=25,\n",
    "    execution=dict(\n",
    "        type='single',\n",
    "        session_config=None,\n",
    "        distributed_spec=None\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "\n",
    "    printi(\"save the mode\")\n",
    "\n",
    "    name_save = \"./saved_models/ppo_model\"\n",
    "    # NOTE: need to check if should create the dir\n",
    "    r.agent.save_model(name_save, append_timestep=False)\n",
    "\n",
    "    # show for plotting\n",
    "    # r.environment.show_control()\n",
    "    # r.environment.show_drag()\n",
    "\n",
    "    # print(sess.run(tf.global_variables()))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring the best model that was learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.restore_model('best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(agent=agent, environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.show_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dolfin import *\n",
    "with XDMFFile(MPI.comm_world, 'u_.xdmf') as file:\n",
    "    file.write(environment.u_, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crrt_drag = environment.history_parameters[\"drag\"].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(crrt_drag, label=\"episode drag\", linewidth=1.2)\n",
    "plt.plot([0, environment.size_history - 1], [environment.inspection_params['line_drag'], environment.inspection_params['line_drag']], label=\"mean drag no control\", linewidth=2.5, linestyle=\"--\")\n",
    "plt.ylabel(\"measured drag D\")\n",
    "plt.xlabel(\"actuation step\")\n",
    "range_drag_plot = environment.inspection_params[\"range_drag_plot\"]\n",
    "plt.legend(loc=2)\n",
    "#plt.ylim(range_drag_plot)\n",
    "plt.tight_layout()\n",
    "#plt.pause(1.0)\n",
    "plt.savefig('drag')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
